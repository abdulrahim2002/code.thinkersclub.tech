<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>raft: add protocol documentation - toydb - An SQL engine written in Rust
</title>
<link rel="icon" type="image/png" href="../favicon.png" />
<link rel="alternate" type="application/atom+xml" title="toydb.git Atom Feed" href="../atom.xml" />
<link rel="alternate" type="application/atom+xml" title="toydb.git Atom Feed (tags)" href="../tags.xml" />
<link rel="stylesheet" type="text/css" href="../style.css" />
</head>
<body>
<table><tr><td><a href="../../"><img src="../logo.png" alt="" width="32" height="32" /></a></td><td><h1>toydb</h1><span class="desc">An SQL engine written in Rust
</span></td></tr><tr class="url"><td></td><td>git clone <a href="https://github.com/erikgrinaker/toydb">https://github.com/erikgrinaker/toydb</a></td></tr><tr><td></td><td>
<a href="../log.html">Log</a> | <a href="../files.html">Files</a> | <a href="../refs.html">Refs</a> | <a href="../file/README.md.html">README</a> | <a href="../file/LICENSE.html">LICENSE</a></td></tr></table>
<hr/>
<div id="content">
<pre><b>commit</b> <a href="../commit/6a5e30e00af35fdac6169e3f2e35aea18b212ca7.html">6a5e30e00af35fdac6169e3f2e35aea18b212ca7</a>
<b>parent</b> <a href="../commit/4c770d184fde5d2642e353758ba9e3967eca726c.html">4c770d184fde5d2642e353758ba9e3967eca726c</a>
<b>Author:</b> Erik Grinaker &lt;<a href="mailto:erik@grinaker.org">erik@grinaker.org</a>&gt;
<b>Date:</b>   Wed, 12 Jun 2024 14:34:23 +0200

raft: add protocol documentation

<b>Diffstat:</b>
<table><tr><td class="M">M</td><td><a href="#h0">src/raft/mod.rs</a></td><td> | </td><td class="num">241</td><td><span class="i">+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</span><span class="d"></span></td></tr>
<tr><td class="M">M</td><td><a href="#h1">src/raft/node.rs</a></td><td> | </td><td class="num">16</td><td><span class="i">+++++++++</span><span class="d">-------</span></td></tr>
</table></pre><pre>2 files changed, 250 insertions(+), 7 deletions(-)
<hr/><b>diff --git a/<a id="h0" href="../file/src/raft/mod.rs.html">src/raft/mod.rs</a> b/<a href="../file/src/raft/mod.rs.html">src/raft/mod.rs</a></b>
<a href="#h0-0" id="h0-0" class="h">@@ -1,3 +1,244 @@
</a><a href="#h0-0-0" id="h0-0-0" class="i">+//! Implements the Raft distributed consensus protocol.
</a><a href="#h0-0-1" id="h0-0-1" class="i">+//!
</a><a href="#h0-0-2" id="h0-0-2" class="i">+//! For details, see Diego Ongaro&#39;s original writings:
</a><a href="#h0-0-3" id="h0-0-3" class="i">+//!
</a><a href="#h0-0-4" id="h0-0-4" class="i">+//! * Raft paper: &lt;https://raft.github.io/raft.pdf&gt;
</a><a href="#h0-0-5" id="h0-0-5" class="i">+//! * Raft thesis: &lt;https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf&gt;
</a><a href="#h0-0-6" id="h0-0-6" class="i">+//! * Raft website: &lt;https://raft.github.io&gt;
</a><a href="#h0-0-7" id="h0-0-7" class="i">+//!
</a><a href="#h0-0-8" id="h0-0-8" class="i">+//! Raft is a protocol for a group of computers to agree on some data -- or more
</a><a href="#h0-0-9" id="h0-0-9" class="i">+//! simply, to replicate the data. It is broadly equivalent to [Paxos] and
</a><a href="#h0-0-10" id="h0-0-10" class="i">+//! [Viewstamped Replication], but more prescriptive and simpler to understand.
</a><a href="#h0-0-11" id="h0-0-11" class="i">+//!
</a><a href="#h0-0-12" id="h0-0-12" class="i">+//! Raft has three main properties:
</a><a href="#h0-0-13" id="h0-0-13" class="i">+//!
</a><a href="#h0-0-14" id="h0-0-14" class="i">+//! * Fault tolerance: the system tolerates node failures as long as a majority
</a><a href="#h0-0-15" id="h0-0-15" class="i">+//!   of nodes (&gt;50%) remain operational.
</a><a href="#h0-0-16" id="h0-0-16" class="i">+//!
</a><a href="#h0-0-17" id="h0-0-17" class="i">+//! * Linearizability (aka strong consistency): once a client write has been
</a><a href="#h0-0-18" id="h0-0-18" class="i">+//!   accepted, it is visible to all clients -- they never see outdated data.
</a><a href="#h0-0-19" id="h0-0-19" class="i">+//!
</a><a href="#h0-0-20" id="h0-0-20" class="i">+//! * Durability: a write is never lost as long as a majority of nodes remain.
</a><a href="#h0-0-21" id="h0-0-21" class="i">+//!
</a><a href="#h0-0-22" id="h0-0-22" class="i">+//! It does this by electing a single leader node which serves client requests
</a><a href="#h0-0-23" id="h0-0-23" class="i">+//! and replicates writes to other nodes. Requests are executed once they have
</a><a href="#h0-0-24" id="h0-0-24" class="i">+//! been confirmed by a strict majority of nodes (a quorum). If a leader fails,
</a><a href="#h0-0-25" id="h0-0-25" class="i">+//! a new leader is elected. Clusters have 3 or more nodes, since a two-node
</a><a href="#h0-0-26" id="h0-0-26" class="i">+//! cluster can&#39;t tolerate failures (1/2 is not a majority and would lead to
</a><a href="#h0-0-27" id="h0-0-27" class="i">+//! split brain).
</a><a href="#h0-0-28" id="h0-0-28" class="i">+//!
</a><a href="#h0-0-29" id="h0-0-29" class="i">+//! Notably, Raft does not provide horizontal scalability. Client requests are
</a><a href="#h0-0-30" id="h0-0-30" class="i">+//! processed by a single leader node which can quickly become a bottleneck, and
</a><a href="#h0-0-31" id="h0-0-31" class="i">+//! each node stores a complete copy of the entire dataset. Systems often handle
</a><a href="#h0-0-32" id="h0-0-32" class="i">+//! this by sharding the data into multiple Raft clusters and using a
</a><a href="#h0-0-33" id="h0-0-33" class="i">+//! distributed transaction protocol across them, but this is out of scope here.
</a><a href="#h0-0-34" id="h0-0-34" class="i">+//!
</a><a href="#h0-0-35" id="h0-0-35" class="i">+//! toyDB follows the Raft paper fairly closely, but, like most implementations,
</a><a href="#h0-0-36" id="h0-0-36" class="i">+//! takes some minor artistic liberties.
</a><a href="#h0-0-37" id="h0-0-37" class="i">+//!
</a><a href="#h0-0-38" id="h0-0-38" class="i">+//! [Paxos]: https://www.microsoft.com/en-us/research/uploads/prod/2016/12/paxos-simple-Copy.pdf
</a><a href="#h0-0-39" id="h0-0-39" class="i">+//! [Viewstamped Replication]: https://pmg.csail.mit.edu/papers/vr-revisited.pdf
</a><a href="#h0-0-40" id="h0-0-40" class="i">+//!
</a><a href="#h0-0-41" id="h0-0-41" class="i">+//! RAFT LOG AND STATE MACHINE
</a><a href="#h0-0-42" id="h0-0-42" class="i">+//! ==========================
</a><a href="#h0-0-43" id="h0-0-43" class="i">+//!
</a><a href="#h0-0-44" id="h0-0-44" class="i">+//! Raft maintains an ordered command log containing arbitrary write commands
</a><a href="#h0-0-45" id="h0-0-45" class="i">+//! submitted by clients. It attempts to reach consensus on this log by
</a><a href="#h0-0-46" id="h0-0-46" class="i">+//! replicating it to a majority of nodes. If successful, the log is considered
</a><a href="#h0-0-47" id="h0-0-47" class="i">+//! committed and immutable up to that point.
</a><a href="#h0-0-48" id="h0-0-48" class="i">+//!
</a><a href="#h0-0-49" id="h0-0-49" class="i">+//! Once committed, the commands in the log are applied sequentially to a local
</a><a href="#h0-0-50" id="h0-0-50" class="i">+//! state machine on each node. Raft itself doesn&#39;t care what the state machine
</a><a href="#h0-0-51" id="h0-0-51" class="i">+//! and commands are -- in toyDB&#39;s case it&#39;s a SQL database, but it could be
</a><a href="#h0-0-52" id="h0-0-52" class="i">+//! anything. Raft simply passes opaque commands to an opaque state machine.
</a><a href="#h0-0-53" id="h0-0-53" class="i">+//!
</a><a href="#h0-0-54" id="h0-0-54" class="i">+//! Each log entry contains an index, the leader&#39;s term (see next section), and
</a><a href="#h0-0-55" id="h0-0-55" class="i">+//! the command. For example, a na√Øve illustration of a toyDB Raft log might be:
</a><a href="#h0-0-56" id="h0-0-56" class="i">+//!
</a><a href="#h0-0-57" id="h0-0-57" class="i">+//! Index | Term | Command
</a><a href="#h0-0-58" id="h0-0-58" class="i">+//! ------|------|------------------------------------------------------
</a><a href="#h0-0-59" id="h0-0-59" class="i">+//!   1   |   1  | CREATE TABLE table (id INT PRIMARY KEY, value STRING)
</a><a href="#h0-0-60" id="h0-0-60" class="i">+//!   2   |   1  | INSERT INTO table VALUES (1, &#39;foo&#39;)
</a><a href="#h0-0-61" id="h0-0-61" class="i">+//!   3   |   2  | UPDATE table SET value = &#39;bar&#39; WHERE id = 1
</a><a href="#h0-0-62" id="h0-0-62" class="i">+//!   4   |   2  | DELETE FROM table WHERE id = 1
</a><a href="#h0-0-63" id="h0-0-63" class="i">+//!
</a><a href="#h0-0-64" id="h0-0-64" class="i">+//! The state machine must be deterministic, such that all nodes will reach the
</a><a href="#h0-0-65" id="h0-0-65" class="i">+//! same identical state. Raft will apply the same commands in the same order
</a><a href="#h0-0-66" id="h0-0-66" class="i">+//! independently on all nodes, but if the commands have non-deterministic
</a><a href="#h0-0-67" id="h0-0-67" class="i">+//! behavior such as random number generation or communication with external
</a><a href="#h0-0-68" id="h0-0-68" class="i">+//! systems it can lead to state divergence causing different results.
</a><a href="#h0-0-69" id="h0-0-69" class="i">+//!
</a><a href="#h0-0-70" id="h0-0-70" class="i">+//! In toyDB, the Raft log is managed by `Log` and stored locally in a
</a><a href="#h0-0-71" id="h0-0-71" class="i">+//! `storage::Engine`. The state machine interface is the `State` trait. See
</a><a href="#h0-0-72" id="h0-0-72" class="i">+//! their documentation for more details.
</a><a href="#h0-0-73" id="h0-0-73" class="i">+//!
</a><a href="#h0-0-74" id="h0-0-74" class="i">+//! LEADER ELECTION
</a><a href="#h0-0-75" id="h0-0-75" class="i">+//! ===============
</a><a href="#h0-0-76" id="h0-0-76" class="i">+//!
</a><a href="#h0-0-77" id="h0-0-77" class="i">+//! Raft nodes can be in one of three states (or roles): follower, candidate,
</a><a href="#h0-0-78" id="h0-0-78" class="i">+//! and leader. toyDB models these as `Node::Follower`, `Node::Candidate`, and
</a><a href="#h0-0-79" id="h0-0-79" class="i">+//! `Node::Leader`.
</a><a href="#h0-0-80" id="h0-0-80" class="i">+//!
</a><a href="#h0-0-81" id="h0-0-81" class="i">+//! * Follower: replicates log entries from a leader. May not know a leader yet.
</a><a href="#h0-0-82" id="h0-0-82" class="i">+//! * Candidate: campaigns for leadership in an election.
</a><a href="#h0-0-83" id="h0-0-83" class="i">+//! * Leader: processes client requests and replicates writes to followers.
</a><a href="#h0-0-84" id="h0-0-84" class="i">+//!
</a><a href="#h0-0-85" id="h0-0-85" class="i">+//! Raft fundamentally relies on a single guarantee: there can be at most one
</a><a href="#h0-0-86" id="h0-0-86" class="i">+//! _valid_ leader at any point in time (old, since-replaced leaders may think
</a><a href="#h0-0-87" id="h0-0-87" class="i">+//! they&#39;re still a leader, e.g. during a network partition, but they won&#39;t be
</a><a href="#h0-0-88" id="h0-0-88" class="i">+//! able to do anything). It enforces this through the leader election protocol.
</a><a href="#h0-0-89" id="h0-0-89" class="i">+//!
</a><a href="#h0-0-90" id="h0-0-90" class="i">+//! Raft divides time into terms, which are monotonically increasing numbers.
</a><a href="#h0-0-91" id="h0-0-91" class="i">+//! Higher terms always take priority over lower terms. There can be at most one
</a><a href="#h0-0-92" id="h0-0-92" class="i">+//! leader in a term, and it can&#39;t change. Nodes keep track of their last known
</a><a href="#h0-0-93" id="h0-0-93" class="i">+//! term and store it on disk (see `Log.set_term()`). Messages between nodes are
</a><a href="#h0-0-94" id="h0-0-94" class="i">+//! tagged with the current term (as `Envelope.term`) -- old terms are ignored,
</a><a href="#h0-0-95" id="h0-0-95" class="i">+//! and future terms cause the node to become a follower in that term.
</a><a href="#h0-0-96" id="h0-0-96" class="i">+//!
</a><a href="#h0-0-97" id="h0-0-97" class="i">+//! Nodes start out as leaderless followers. If they receive a message from a
</a><a href="#h0-0-98" id="h0-0-98" class="i">+//! leader (in a current or future term), they follow it. Otherwise, they wait
</a><a href="#h0-0-99" id="h0-0-99" class="i">+//! out the election timeout (a few seconds), become candidates, and hold a
</a><a href="#h0-0-100" id="h0-0-100" class="i">+//! leader election.
</a><a href="#h0-0-101" id="h0-0-101" class="i">+//!
</a><a href="#h0-0-102" id="h0-0-102" class="i">+//! Candidates increase their term by 1 and send `Message::Campaign` to all
</a><a href="#h0-0-103" id="h0-0-103" class="i">+//! nodes, requesting their vote. Nodes respond with `Message::CampaignResponse`
</a><a href="#h0-0-104" id="h0-0-104" class="i">+//! saying whether a vote was granted. A node can only grant a single vote in a
</a><a href="#h0-0-105" id="h0-0-105" class="i">+//! term (stored to disk via `Log.set_term()`), on a first-come first-serve
</a><a href="#h0-0-106" id="h0-0-106" class="i">+//! basis, and candidates implicitly vote for themselves.
</a><a href="#h0-0-107" id="h0-0-107" class="i">+//!
</a><a href="#h0-0-108" id="h0-0-108" class="i">+//! When a candidate receives a majority of votes (&gt;50%), it becomes leader. It
</a><a href="#h0-0-109" id="h0-0-109" class="i">+//! sends a `Message::Heartbeat` to all nodes asserting its leadership, and all
</a><a href="#h0-0-110" id="h0-0-110" class="i">+//! nodes become followers when they receive it (regardless of who they voted
</a><a href="#h0-0-111" id="h0-0-111" class="i">+//! for). Leaders continue to send periodic heartbeats every second or so. The
</a><a href="#h0-0-112" id="h0-0-112" class="i">+//! new leader also appends an empty entry to its log in order to safely commit
</a><a href="#h0-0-113" id="h0-0-113" class="i">+//! all entries from previous terms (Raft paper section 5.4.2).
</a><a href="#h0-0-114" id="h0-0-114" class="i">+//!
</a><a href="#h0-0-115" id="h0-0-115" class="i">+//! The new leader must have all committed entries in its log (or the cluster
</a><a href="#h0-0-116" id="h0-0-116" class="i">+//! would lose data). To ensure this, there is one additional condition for
</a><a href="#h0-0-117" id="h0-0-117" class="i">+//! granting a vote: the candidate&#39;s log must be at least as up-to-date as the
</a><a href="#h0-0-118" id="h0-0-118" class="i">+//! voter. Because an entry must be replicated to a majority before being
</a><a href="#h0-0-119" id="h0-0-119" class="i">+//! committed, this ensures a candidate can only win a majority of votes if its
</a><a href="#h0-0-120" id="h0-0-120" class="i">+//! log is up-to-date with all committed entries (Raft paper section 5.4.1).
</a><a href="#h0-0-121" id="h0-0-121" class="i">+//!
</a><a href="#h0-0-122" id="h0-0-122" class="i">+//! It&#39;s possible that no candidate wins an election, for example due to a tie
</a><a href="#h0-0-123" id="h0-0-123" class="i">+//! or a majority of nodes being offline. After an election timeout passes,
</a><a href="#h0-0-124" id="h0-0-124" class="i">+//! candidates will again bump their term and start a new election, until a
</a><a href="#h0-0-125" id="h0-0-125" class="i">+//! leader can be established. To avoid frequent ties, nodes use different,
</a><a href="#h0-0-126" id="h0-0-126" class="i">+//! randomized election timeouts (Raft paper section 5.2).
</a><a href="#h0-0-127" id="h0-0-127" class="i">+//!
</a><a href="#h0-0-128" id="h0-0-128" class="i">+//! Similarly, if a follower doesn&#39;t hear from a leader in an election timeout
</a><a href="#h0-0-129" id="h0-0-129" class="i">+//! interval, it will become candidate and hold another election. The periodic
</a><a href="#h0-0-130" id="h0-0-130" class="i">+//! leader heartbeats prevent this as long as the leader is running and
</a><a href="#h0-0-131" id="h0-0-131" class="i">+//! connected. A node that becomes disconnected from the leader will continually
</a><a href="#h0-0-132" id="h0-0-132" class="i">+//! hold new elections by itself until the network heals, at which point a new
</a><a href="#h0-0-133" id="h0-0-133" class="i">+//! election will be held in its term (disrupting the current leader).
</a><a href="#h0-0-134" id="h0-0-134" class="i">+//!
</a><a href="#h0-0-135" id="h0-0-135" class="i">+//! REPLICATION AND CONSENSUS
</a><a href="#h0-0-136" id="h0-0-136" class="i">+//! =========================
</a><a href="#h0-0-137" id="h0-0-137" class="i">+//!
</a><a href="#h0-0-138" id="h0-0-138" class="i">+//! When the leader receives a client write request, it appends the command to
</a><a href="#h0-0-139" id="h0-0-139" class="i">+//! its local log via `Log.append()`, and sends the log entry to all peers in
</a><a href="#h0-0-140" id="h0-0-140" class="i">+//! a `Message::Append`. Followers will attempt to durably append the entry to
</a><a href="#h0-0-141" id="h0-0-141" class="i">+//! their local logs and respond with `Message::AppendResponse`.
</a><a href="#h0-0-142" id="h0-0-142" class="i">+//!
</a><a href="#h0-0-143" id="h0-0-143" class="i">+//! Once a majority have acknowledged the append, the leader commits the entry
</a><a href="#h0-0-144" id="h0-0-144" class="i">+//! via `Log.commit()` and applies it to its local state machine, returning the
</a><a href="#h0-0-145" id="h0-0-145" class="i">+//! result to the client. It will inform followers about the commit in the next
</a><a href="#h0-0-146" id="h0-0-146" class="i">+//! heartbeat as `Message::Heartbeat.commit_index` so they can apply it too, but
</a><a href="#h0-0-147" id="h0-0-147" class="i">+//! this is not necessary for correctness (they will commit and apply it if they
</a><a href="#h0-0-148" id="h0-0-148" class="i">+//! become leader, otherwise they have no need for applying it).
</a><a href="#h0-0-149" id="h0-0-149" class="i">+//!
</a><a href="#h0-0-150" id="h0-0-150" class="i">+//! Followers may not be able to append the entry to their log -- they may be
</a><a href="#h0-0-151" id="h0-0-151" class="i">+//! unreachable, lag behind the leader, or have divergent logs (see Raft paper
</a><a href="#h0-0-152" id="h0-0-152" class="i">+//! section 5.3). The `Append` contains the index and term of the log entry
</a><a href="#h0-0-153" id="h0-0-153" class="i">+//! immediately before the replicated entry as `base_index` and `base_term`. An
</a><a href="#h0-0-154" id="h0-0-154" class="i">+//! index/term pair uniquely identifies a command, and if two logs have the same
</a><a href="#h0-0-155" id="h0-0-155" class="i">+//! index/term pair then the logs are identical up to and including that entry
</a><a href="#h0-0-156" id="h0-0-156" class="i">+//! (Raft paper section 5.3). If the base index/term matches the follower&#39;s log,
</a><a href="#h0-0-157" id="h0-0-157" class="i">+//! it appends the entry (potentially replacing any conflicting entries),
</a><a href="#h0-0-158" id="h0-0-158" class="i">+//! otherwise it rejects it.
</a><a href="#h0-0-159" id="h0-0-159" class="i">+//!
</a><a href="#h0-0-160" id="h0-0-160" class="i">+//! When a follower rejects an append, the leader must try to find a common log
</a><a href="#h0-0-161" id="h0-0-161" class="i">+//! entry that exists in both its and the follower&#39;s log where it can resume
</a><a href="#h0-0-162" id="h0-0-162" class="i">+//! replication. It does this by sending `Message::Append` probes only
</a><a href="#h0-0-163" id="h0-0-163" class="i">+//! containing a base index/term but no entries -- it will continue to probe
</a><a href="#h0-0-164" id="h0-0-164" class="i">+//! decreasing indexes one by one until the follower responds with a match, then
</a><a href="#h0-0-165" id="h0-0-165" class="i">+//! send an `Append` with the missing entries (Raft paper section 5.3). It keeps
</a><a href="#h0-0-166" id="h0-0-166" class="i">+//! track of each follower&#39;s `match_index` and `next_index` in a `Progress`
</a><a href="#h0-0-167" id="h0-0-167" class="i">+//! struct to manage this.
</a><a href="#h0-0-168" id="h0-0-168" class="i">+//!
</a><a href="#h0-0-169" id="h0-0-169" class="i">+//! In case `Append` messages or responses are lost, leaders also send their
</a><a href="#h0-0-170" id="h0-0-170" class="i">+//! `last_index` and term in each `Heartbeat`. If followers don&#39;t have that
</a><a href="#h0-0-171" id="h0-0-171" class="i">+//! index/term pair in their log, they&#39;ll say so in the `HeartbeatResponse` and
</a><a href="#h0-0-172" id="h0-0-172" class="i">+//! the leader can begin probing their logs as with append rejections.
</a><a href="#h0-0-173" id="h0-0-173" class="i">+//!
</a><a href="#h0-0-174" id="h0-0-174" class="i">+//! CLIENT REQUESTS
</a><a href="#h0-0-175" id="h0-0-175" class="i">+//! ===============
</a><a href="#h0-0-176" id="h0-0-176" class="i">+//!
</a><a href="#h0-0-177" id="h0-0-177" class="i">+//! Client requests are submitted as `Message::ClientRequest` to the local Raft
</a><a href="#h0-0-178" id="h0-0-178" class="i">+//! node. They are only processed on the leader, but followers will proxy them
</a><a href="#h0-0-179" id="h0-0-179" class="i">+//! to the leader (Raft thesis section 6.2). To avoid complications with message
</a><a href="#h0-0-180" id="h0-0-180" class="i">+//! replays (Raft thesis section 6.3), requests are not retried internally, and
</a><a href="#h0-0-181" id="h0-0-181" class="i">+//! are explicitly aborted with `Error::Abort` on leader/term changes as well as
</a><a href="#h0-0-182" id="h0-0-182" class="i">+//! elections.
</a><a href="#h0-0-183" id="h0-0-183" class="i">+//!
</a><a href="#h0-0-184" id="h0-0-184" class="i">+//! Write requests, `Request::Write`, are appended to the Raft log and
</a><a href="#h0-0-185" id="h0-0-185" class="i">+//! replicated. The leader keeps track of the request and its log index in a
</a><a href="#h0-0-186" id="h0-0-186" class="i">+//! `Write` struct. Once the command is committed and applied to the local state
</a><a href="#h0-0-187" id="h0-0-187" class="i">+//! machine, the leader looks up the write request by its log index and sends
</a><a href="#h0-0-188" id="h0-0-188" class="i">+//! the result to the client. Deterministic errors (e.g. foreign key violations)
</a><a href="#h0-0-189" id="h0-0-189" class="i">+//! are also returned to the client, but non-deterministic errors (e.g. IO
</a><a href="#h0-0-190" id="h0-0-190" class="i">+//! errors) must panic the node to avoid replica state divergence.
</a><a href="#h0-0-191" id="h0-0-191" class="i">+//!
</a><a href="#h0-0-192" id="h0-0-192" class="i">+//! Read requests, `Request::Read`, are only executed on the leader and don&#39;t
</a><a href="#h0-0-193" id="h0-0-193" class="i">+//! need to be replicated via the Raft log. However, to ensure linearizability,
</a><a href="#h0-0-194" id="h0-0-194" class="i">+//! the leader has to confirm with a quorum that it&#39;s actually still the leader.
</a><a href="#h0-0-195" id="h0-0-195" class="i">+//! Otherwise, it&#39;s possible that a new leader has been elected elsewhere and
</a><a href="#h0-0-196" id="h0-0-196" class="i">+//! executed writes without us knowing about it. It does this by assigning an
</a><a href="#h0-0-197" id="h0-0-197" class="i">+//! incrementing sequence number to each read, keeping track of the request in a
</a><a href="#h0-0-198" id="h0-0-198" class="i">+//! `Read` struct, and immediately sending a heartbeat with the latest sequence
</a><a href="#h0-0-199" id="h0-0-199" class="i">+//! number in `Heartbeat.read_seq`. Followers include the `read_seq` in their
</a><a href="#h0-0-200" id="h0-0-200" class="i">+//! `HeartbeatResponse`, and once a quorum have confirmed a sequence number the
</a><a href="#h0-0-201" id="h0-0-201" class="i">+//! read is executed and the result returned to the client.
</a><a href="#h0-0-202" id="h0-0-202" class="i">+//!
</a><a href="#h0-0-203" id="h0-0-203" class="i">+//! IMPLEMENTATION CAVEATS
</a><a href="#h0-0-204" id="h0-0-204" class="i">+//! ======================
</a><a href="#h0-0-205" id="h0-0-205" class="i">+//!
</a><a href="#h0-0-206" id="h0-0-206" class="i">+//! For simplicity, toyDB implements the bare minimum for a functional and
</a><a href="#h0-0-207" id="h0-0-207" class="i">+//! correct Raft protocol, and omits several advanced mechanisms that would be
</a><a href="#h0-0-208" id="h0-0-208" class="i">+//! needed for a real production system. In particular:
</a><a href="#h0-0-209" id="h0-0-209" class="i">+//!
</a><a href="#h0-0-210" id="h0-0-210" class="i">+//! * No leases: for linearizability, every read request requires the leader to
</a><a href="#h0-0-211" id="h0-0-211" class="i">+//!   confirm with followers that it&#39;s still the leader. This could be avoided
</a><a href="#h0-0-212" id="h0-0-212" class="i">+//!   with a leader lease for a predefined time interval (Raft paper section 8,
</a><a href="#h0-0-213" id="h0-0-213" class="i">+//!   Raft thesis section 6.3).
</a><a href="#h0-0-214" id="h0-0-214" class="i">+//!
</a><a href="#h0-0-215" id="h0-0-215" class="i">+//! * No cluster membership changes: to add or remove nodes, the entire cluster
</a><a href="#h0-0-216" id="h0-0-216" class="i">+//!   must be stopped and restarted with the new configuration, otherwise it
</a><a href="#h0-0-217" id="h0-0-217" class="i">+//!   risks multiple leaders (Raft paper section 6).
</a><a href="#h0-0-218" id="h0-0-218" class="i">+//!
</a><a href="#h0-0-219" id="h0-0-219" class="i">+//! * No snapshots: new or lagging nodes must be caught up by replicating and
</a><a href="#h0-0-220" id="h0-0-220" class="i">+//!   replaying the entire log, instead of sending a state machine snapshot
</a><a href="#h0-0-221" id="h0-0-221" class="i">+//!   (Raft paper section 7).
</a><a href="#h0-0-222" id="h0-0-222" class="i">+//!
</a><a href="#h0-0-223" id="h0-0-223" class="i">+//! * No log truncation: because snapshots aren&#39;t supported, the entire Raft
</a><a href="#h0-0-224" id="h0-0-224" class="i">+//!   log must be retained forever in order to catch up new/lagging nodes,
</a><a href="#h0-0-225" id="h0-0-225" class="i">+//!   leading to excessive storage use (Raft paper section 7).
</a><a href="#h0-0-226" id="h0-0-226" class="i">+//!
</a><a href="#h0-0-227" id="h0-0-227" class="i">+//! * No pre-vote or check-quorum: a node that&#39;s partially partitioned (can
</a><a href="#h0-0-228" id="h0-0-228" class="i">+//!   reach some but not all nodes) can cause persistent unavailability with
</a><a href="#h0-0-229" id="h0-0-229" class="i">+//!   spurious elections or heartbeats. A node rejoining after a partition can
</a><a href="#h0-0-230" id="h0-0-230" class="i">+//!   also temporarily disrupt a leader. This requires additional pre-vote and
</a><a href="#h0-0-231" id="h0-0-231" class="i">+//!   check-quorum protocol extensions (Raft thesis section 4.2.3 and 9.6).
</a><a href="#h0-0-232" id="h0-0-232" class="i">+//!
</a><a href="#h0-0-233" id="h0-0-233" class="i">+//! * No request retries: client requests will not be retried on leader changes
</a><a href="#h0-0-234" id="h0-0-234" class="i">+//!   or message loss, and will be aggressively aborted, to ignore problems
</a><a href="#h0-0-235" id="h0-0-235" class="i">+//!   related to message replay (Raft thesis section 6.3).
</a><a href="#h0-0-236" id="h0-0-236" class="i">+//!
</a><a href="#h0-0-237" id="h0-0-237" class="i">+//! * No reject hints: if a follower has a divergent log, the leader will probe
</a><a href="#h0-0-238" id="h0-0-238" class="i">+//!   entries one by one until a match is found. The replication protocol could
</a><a href="#h0-0-239" id="h0-0-239" class="i">+//!   instead be extended with rejection hints (Raft paper section 5.3).
</a><a href="#h0-0-240" id="h0-0-240" class="i">+
</a> mod log;
 mod message;
 mod node;
<b>diff --git a/<a id="h1" href="../file/src/raft/node.rs.html">src/raft/node.rs</a> b/<a href="../file/src/raft/node.rs.html">src/raft/node.rs</a></b>
<a href="#h1-0" id="h1-0" class="h">@@ -40,11 +40,13 @@ impl Default for Options {
</a>     }
 }
 
<a href="#h1-0-3" id="h1-0-3" class="d">-/// A Raft node, with a dynamic role. The node is driven synchronously by
</a><a href="#h1-0-4" id="h1-0-4" class="d">-/// processing inbound messages via `step()` or by advancing time via `tick()`.
</a><a href="#h1-0-5" id="h1-0-5" class="d">-/// These methods consume the node and return a new one with a possibly
</a><a href="#h1-0-6" id="h1-0-6" class="d">-/// different role. Outbound messages are sent via the given `tx` channel, and
</a><a href="#h1-0-7" id="h1-0-7" class="d">-/// must be delivered to the remote peers.
</a><a href="#h1-0-8" id="h1-0-8" class="i">+/// A Raft node with a dynamic role. This implements the Raft distributed
</a><a href="#h1-0-9" id="h1-0-9" class="i">+/// consensus protocol, see the `raft` module documentation for more info.
</a><a href="#h1-0-10" id="h1-0-10" class="i">+///
</a><a href="#h1-0-11" id="h1-0-11" class="i">+/// The node is driven synchronously by processing inbound messages via `step()`
</a><a href="#h1-0-12" id="h1-0-12" class="i">+/// and by advancing time via `tick()`. These methods consume the node and
</a><a href="#h1-0-13" id="h1-0-13" class="i">+/// return a new one with a possibly different role. Outbound messages are sent
</a><a href="#h1-0-14" id="h1-0-14" class="i">+/// via the given `tx` channel, and must be delivered to peers or clients.
</a> ///
 /// This enum is the public interface to the node, with a closed set of roles.
 /// It wraps the `RawNode&lt;Role&gt;` types, which implement the actual node logic.
<a href="#h1-1" id="h1-1" class="h">@@ -291,8 +293,8 @@ impl RawNode&lt;Follower&gt; {
</a>         node.role.election_timeout = node.random_election_timeout();
 
         // Apply any pending entries following restart. Unlike the Raft log,
<a href="#h1-1-3" id="h1-1-3" class="d">-        // state machines writes are not flushed to durable storage, so a tail
</a><a href="#h1-1-4" id="h1-1-4" class="d">-        // of writes may be lost if the OS crashes or restarts.
</a><a href="#h1-1-5" id="h1-1-5" class="i">+        // state machine writes are not flushed to durable storage, so a tail of
</a><a href="#h1-1-6" id="h1-1-6" class="i">+        // writes may be lost if the OS crashes or restarts.
</a>         node.maybe_apply()?;
         Ok(node)
     }
</pre>
</div>
</body>
</html>
